{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1bce131",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"In this paper, we introduce a novel graph RAG method for applying LLMs to the medical domain, which we refer to as Medical Graph RAG (MedRAG). This technique improves LLM performance in the medical domain by response queries with grounded source citations and clear interpretations of medical terminology, boosting the transparency and interpretability of the results. This approach involves a three-tier hierarchical graph construction method. Initially, we use documents provided by users as our top-level source to extract entities. These entities are then linked to a second level consisting of more basic entities previously abstracted from credible medical books and papers. Subsequently, these entities are connected to a third level—the fundamental medical dictionary graph—that provides detailed explanations of each medical term and their semantic relationships. We then construct a comprehensive graph at the highest level by linking entities based on their content and hierarchical connections. This method ensures that the knowledge can be traced back to its sources and the results are factually accurate.\n",
    "\n",
    "To respond to user queries, we implement a U-retrieve strategy that combines top-down retrieval with bottom-up response generation. The process begins by structuring the query using predefined medical tags and indexing them through the graphs in a top-down manner. The system then generates responses based on these queries, pulling from meta-graphs—nodes retrieved along with their TopK related nodes and relationships—and summarizing the information into a detailed response. This technique maintains a balance between global context awareness and the contextual limitations inherent in LLMs.\n",
    "\n",
    "Our medical graph RAG provides Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. The results provides the provenance, or source grounding information, as it generates each response, and demonstrates that an answer is grounded in the dataset. Having the cited source for each assertion readily available also enables a human user to quickly and accurately audit the LLM’s output directly against the original source material. It is super useful in the field of medicine that security is very important, and each of the reasoning should be evidence-based. By using such a method, we construct an evidence-based Medical LLM that the clinician could easiely check the source of the reasoning and calibrate the model response to ensure the safty usage of llm in the clinical senarios.\n",
    "\n",
    "To evaluate our medical graph RAG, we implemented the method on several popular open and closed-source LLMs, including ChatGPT OpenAI (2023a) and LLaMA Touvron et al. (2023), testing them across mainstream medical Q&A benchmarks such as PubMedQA Jin et al. (2019), MedMCQA Pal et al. (2022), and USMLE Kung et al. (2023). For the RAG process, we supplied a comprehensive medical dictionary as the foundational knowledge layer, the UMLS medical knowledge graph Lindberg et al. (1993) as the foundamental layer detailing semantic relationships, and a curated MedC-K dataset Wu et al. (2023) —comprising the latest medical papers and books—as the intermediate level of data to simulate user-provided private data. Our experiments demonstrate that our model significantly enhances the performance of general-purpose LLMs on medical questions. Remarkably, it even surpasses many fine-tuned or specially trained LLMs on medical corpora, solely using the RAG approach without additional training.\n",
    "\"\"\"\n",
    "\n",
    "from llama_index.core import Document\n",
    "\n",
    "documents = [Document(text=text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7beb150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "from llama_index.core import Settings\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "load_dotenv()\n",
    "\n",
    "embed_model = GoogleGenAIEmbedding(model_name=\"text-embedding-004\")\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    # api_key=\"some key\",  # uses GOOGLE_API_KEY env var by default\n",
    ")\n",
    "Settings.llm = GoogleGenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64b17a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser.topic import TopicNodeParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72fdd143",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = TopicNodeParser.from_defaults(\n",
    "    llm=llm,\n",
    "    max_chunk_size=1000,\n",
    "    similarity_method=\"llm\",  # can be \"llm\" or \"embedding\"\n",
    "    window_size=2, \n",
    "    embed_model=embed_model # paper suggests window_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e29f790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1da496a96124f3d82dc4073e1cd32aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ClientError",
     "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '25s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m nodes=\u001b[43mnode_parser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_nodes_from_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91912\\anaconda3\\Lib\\site-packages\\llama_index\\core\\node_parser\\interface.py:165\u001b[39m, in \u001b[36mNodeParser.get_nodes_from_documents\u001b[39m\u001b[34m(self, documents, show_progress, **kwargs)\u001b[39m\n\u001b[32m    160\u001b[39m doc_id_to_document = {doc.id_: doc \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents}\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    163\u001b[39m     CBEventType.NODE_PARSING, payload={EventPayload.DOCUMENTS: documents}\n\u001b[32m    164\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     nodes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     nodes = \u001b[38;5;28mself\u001b[39m._postprocess_parsed_nodes(nodes, doc_id_to_document)\n\u001b[32m    168\u001b[39m     event.on_end({EventPayload.NODES: nodes})\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91912\\anaconda3\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91912\\anaconda3\\Lib\\site-packages\\llama_index\\node_parser\\topic\\base.py:120\u001b[39m, in \u001b[36mTopicNodeParser._parse_nodes\u001b[39m\u001b[34m(self, nodes, show_progress, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m nodes_with_progress = get_tqdm_iterable(nodes, show_progress, \u001b[33m\"\u001b[39m\u001b[33mParsing nodes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes_with_progress:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     nodes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_topic_based_nodes_from_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     all_nodes.extend(nodes)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m all_nodes\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91912\\anaconda3\\Lib\\site-packages\\llama_index\\node_parser\\topic\\base.py:255\u001b[39m, in \u001b[36mTopicNodeParser.build_topic_based_nodes_from_documents\u001b[39m\u001b[34m(self, documents)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[32m    254\u001b[39m     paragraphs = \u001b[38;5;28mself\u001b[39m.split_into_paragraphs(doc.text)\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m     chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msemantic_chunking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparagraphs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m     nodes = build_nodes_from_splits(\n\u001b[32m    257\u001b[39m         chunks,\n\u001b[32m    258\u001b[39m         doc,\n\u001b[32m    259\u001b[39m         id_func=\u001b[38;5;28mself\u001b[39m.id_func,\n\u001b[32m    260\u001b[39m     )\n\u001b[32m    261\u001b[39m     all_nodes.extend(nodes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91912\\anaconda3\\Lib\\site-packages\\llama_index\\node_parser\\topic\\base.py:213\u001b[39m, in \u001b[36mTopicNodeParser.semantic_chunking\u001b[39m\u001b[34m(self, paragraphs)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_chunk:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.similarity_method == \u001b[33m\"\u001b[39m\u001b[33mllm\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m         is_same_topic = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mis_same_topic_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.similarity_method == \u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    215\u001b[39m         is_same_topic = \u001b[38;5;28mself\u001b[39m.is_same_topic_embedding(\n\u001b[32m    216\u001b[39m             current_chunk, prop\n\u001b[32m    217\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91912\\anaconda3\\Lib\\site-packages\\llama_index\\node_parser\\topic\\base.py:167\u001b[39m, in \u001b[36mTopicNodeParser.is_same_topic_llm\u001b[39m\u001b[34m(self, current_chunk, new_proposition)\u001b[39m\n\u001b[32m    158\u001b[39m current_text = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(current_chunk)\n\u001b[32m    159\u001b[39m messages = [\n\u001b[32m    160\u001b[39m     ChatMessage(role=\u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, content=TOPIC_CLASSIFICATION_SYSTEM_PROMPT),\n\u001b[32m    161\u001b[39m     ChatMessage(\n\u001b[32m   (...)\u001b[39m\u001b[32m    164\u001b[39m     ),\n\u001b[32m    165\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msame topic\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(response).lower()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91912\\anaconda3\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91912\\anaconda3\\Lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:173\u001b[39m, in \u001b[36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[39m\u001b[34m(_self, messages, **kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m event_id = callback_manager.on_event_start(\n\u001b[32m    165\u001b[39m     CBEventType.LLM,\n\u001b[32m    166\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m    170\u001b[39m     },\n\u001b[32m    171\u001b[39m )\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     f_return_val = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    175\u001b[39m     callback_manager.on_event_end(\n\u001b[32m    176\u001b[39m         CBEventType.LLM,\n\u001b[32m    177\u001b[39m         payload={EventPayload.EXCEPTION: e},\n\u001b[32m    178\u001b[39m         event_id=event_id,\n\u001b[32m    179\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91912\\anaconda3\\Lib\\site-packages\\llama_index\\llms\\google_genai\\base.py:244\u001b[39m, in \u001b[36mGoogleGenAI.chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    242\u001b[39m next_msg, chat_kwargs = prepare_chat_params(\u001b[38;5;28mself\u001b[39m.model, messages, **params)\n\u001b[32m    243\u001b[39m chat = \u001b[38;5;28mself\u001b[39m._client.chats.create(**chat_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m response = \u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_msg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chat_from_gemini_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91912\\anaconda3\\Lib\\site-packages\\google\\genai\\chats.py:259\u001b[39m, in \u001b[36mChat.send_message\u001b[39m\u001b[34m(self, message, config)\u001b[39m\n\u001b[32m    254\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    255\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMessage must be a valid part type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes.PartUnion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    256\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes.PartUnionDict\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m   )\n\u001b[32m    258\u001b[39m input_content = t.t_content(\u001b[38;5;28mself\u001b[39m._modules._api_client, message)\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_modules\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_curated_history\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_content\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m model_output = (\n\u001b[32m    265\u001b[39m     [response.candidates[\u001b[32m0\u001b[39m].content]\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.candidates \u001b[38;5;129;01mand\u001b[39;00m response.candidates[\u001b[32m0\u001b[39m].content\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    268\u001b[39m )\n\u001b[32m    269\u001b[39m automatic_function_calling_history = (\n\u001b[32m    270\u001b[39m     response.automatic_function_calling_history\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.automatic_function_calling_history\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    273\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91912\\anaconda3\\Lib\\site-packages\\google\\genai\\models.py:4799\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4797\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   4798\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m4799\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4800\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m   4801\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4802\u001b[39m   logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is done.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   4803\u001b[39m   remaining_remote_calls_afc -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91912\\anaconda3\\Lib\\site-packages\\google\\genai\\models.py:3775\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   3772\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   3773\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m3775\u001b[39m response_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3776\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   3777\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3779\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n\u001b[32m   3780\u001b[39m   response_dict = _GenerateContentResponse_from_vertex(\n\u001b[32m   3781\u001b[39m       \u001b[38;5;28mself\u001b[39m._api_client, response_dict\n\u001b[32m   3782\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91912\\anaconda3\\Lib\\site-packages\\google\\genai\\_api_client.py:726\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    717\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    718\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    721\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    722\u001b[39m ) -> Union[BaseResponse, Any]:\n\u001b[32m    723\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m    724\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m    725\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    727\u001b[39m   json_response = response.json\n\u001b[32m    728\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91912\\anaconda3\\Lib\\site-packages\\google\\genai\\_api_client.py:655\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    648\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m    649\u001b[39m       method=http_request.method,\n\u001b[32m    650\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    653\u001b[39m       timeout=http_request.timeout,\n\u001b[32m    654\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    657\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    658\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91912\\anaconda3\\Lib\\site-packages\\google\\genai\\errors.py:101\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m     99\u001b[39m status_code = response.status_code\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    103\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}, 'quotaValue': '15'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '25s'}]}}"
     ]
    }
   ],
   "source": [
    "nodes=node_parser.get_nodes_from_documents(documents,show_progress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
